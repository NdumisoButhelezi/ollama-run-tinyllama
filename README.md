
This guide covers both **Windows** and **Linux** setups for running models like **TinyLlama** and **Mistral** using **Ollama**.

---

## ğŸªŸ 1. Windows Setup (Easiest for Beginners)

### Step 1 â€“ Download Ollama
1. Go to ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)  
2. Download the **Windows installer**.  
3. Install it (just like a normal app).

### Step 2 â€“ Open Terminal
- Open **PowerShell** or **Command Prompt**.  
- Type:  
```powershell
ollama
```
If you see Ollamaâ€™s help menu â†’ âœ… itâ€™s installed.

### Step 3 â€“ Run Models
Pull & run **TinyLlama**:
```powershell
ollama run tinyllama
```

Pull & run **Mistral**:
```powershell
ollama run mistral
```

ğŸ‘‰ First run will **download the model** (hundreds of MB to a few GB depending on model). After that, itâ€™s instant.

---

## ğŸ§ 2. Linux Setup (More Efficient for AI Work)

### Step 1 â€“ Update System
Open terminal:
```bash
sudo apt update && sudo apt upgrade -y
```

### Step 2 â€“ Install Ollama
Run the official install script:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Step 3 â€“ Verify
Check Ollama is installed:
```bash
ollama
```

### Step 4 â€“ Run Models
Run **TinyLlama**:
```bash
ollama run tinyllama
```

Run **Mistral**:
```bash
ollama run mistral
```

---

## âš¡ Optional: Web UI for Easier Use
If you prefer a **chat interface in your browser** (instead of terminal), install:

```bash
git clone https://github.com/ollama-webui/ollama-webui.git
cd ollama-webui
./start.sh   # Linux
```
On Windows youâ€™d run the included `.bat` file.

---

## âœ… Recommendations for i5 + 12GB RAM
- Try **TinyLlama** first â†’ lightweight, fast, great for testing.  
- Then try **Mistral 7B** â†’ heavier but smarter (may be slower on CPU).  
- For lighter use, consider **quantized models** (4-bit) to save memory.  
